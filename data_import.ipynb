{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96QFTeizJSTx"
   },
   "source": [
    "# Multiclass Text Tagging with Doc2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClR8xzqTC_V5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numba\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jqey4KseJmbj"
   },
   "source": [
    "## Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    \n",
    "    text = text.replace(\"</p>\", \" \")\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "            \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGbq5eZ_k0_L"
   },
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"data/Train.csv\", index_col=\"Id\",chunksize=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "2KqdqNRvlD1M",
    "outputId": "61cb4720-c5f3-47fc-aeb6-acf22843aa72",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0\n",
      " - chunk shape: 60000\n",
      "Processing chunk 1\n",
      " - chunk shape: 60000\n",
      "Processing chunk 2\n",
      " - chunk shape: 60000\n",
      "Processing chunk 3\n",
      " - chunk shape: 60000\n",
      "Processing chunk 4\n",
      " - chunk shape: 60000\n",
      "Processing chunk 5\n",
      " - chunk shape: 60000\n",
      "Processing chunk 6\n",
      " - chunk shape: 60000\n",
      "Processing chunk 7\n",
      " - chunk shape: 60000\n",
      "Processing chunk 8\n",
      " - chunk shape: 60000\n",
      "Processing chunk 9\n",
      " - chunk shape: 60000\n",
      "Processing chunk 10\n",
      " - chunk shape: 60000\n",
      "Processing chunk 11\n",
      " - chunk shape: 60000\n",
      "Processing chunk 12\n",
      " - chunk shape: 60000\n",
      "Processing chunk 13\n",
      " - chunk shape: 60000\n",
      "Processing chunk 14\n",
      " - chunk shape: 60000\n",
      "Processing chunk 15\n",
      " - chunk shape: 60000\n",
      "Processing chunk 16\n",
      " - chunk shape: 60000\n",
      "Processing chunk 17\n",
      " - chunk shape: 60000\n",
      "Processing chunk 18\n",
      " - chunk shape: 60000\n",
      "Processing chunk 19\n",
      " - chunk shape: 60000\n",
      "Processing chunk 20\n",
      " - chunk shape: 60000\n",
      "Processing chunk 21\n",
      " - chunk shape: 60000\n",
      "Processing chunk 22\n",
      " - chunk shape: 60000\n",
      "Processing chunk 23\n",
      " - chunk shape: 60000\n",
      "Processing chunk 24\n",
      " - chunk shape: 60000\n",
      "Processing chunk 25\n",
      " - chunk shape: 60000\n",
      "Processing chunk 26\n",
      " - chunk shape: 60000\n",
      "Processing chunk 27\n",
      " - chunk shape: 60000\n",
      "Processing chunk 28\n",
      " - chunk shape: 60000\n",
      "Processing chunk 29\n",
      " - chunk shape: 60000\n",
      "Processing chunk 30\n",
      " - chunk shape: 60000\n",
      "Processing chunk 31\n",
      " - chunk shape: 60000\n",
      "Processing chunk 32\n",
      " - chunk shape: 60000\n",
      "Processing chunk 33\n",
      " - chunk shape: 60000\n",
      "Processing chunk 34\n",
      " - chunk shape: 60000\n",
      "Processing chunk 35\n",
      " - chunk shape: 60000\n",
      "Processing chunk 36\n",
      " - chunk shape: 60000\n",
      "Processing chunk 37\n",
      " - chunk shape: 60000\n",
      "Processing chunk 38\n",
      " - chunk shape: 60000\n",
      "Processing chunk 39\n",
      " - chunk shape: 60000\n",
      "Processing chunk 40\n",
      " - chunk shape: 60000\n",
      "Processing chunk 41\n",
      " - chunk shape: 60000\n",
      "Processing chunk 42\n",
      " - chunk shape: 60000\n",
      "Processing chunk 43\n",
      " - chunk shape: 60000\n",
      "Processing chunk 44\n",
      " - chunk shape: 60000\n",
      "Processing chunk 45\n",
      " - chunk shape: 60000\n",
      "Processing chunk 46\n",
      " - chunk shape: 60000\n",
      "Processing chunk 47\n",
      " - chunk shape: 60000\n",
      "Processing chunk 48\n",
      " - chunk shape: 60000\n",
      "Processing chunk 49\n",
      " - chunk shape: 60000\n",
      "Processing chunk 50\n",
      " - chunk shape: 60000\n",
      "Processing chunk 51\n",
      " - chunk shape: 60000\n",
      "Processing chunk 52\n",
      " - chunk shape: 60000\n",
      "Processing chunk 53\n",
      " - chunk shape: 60000\n",
      "Processing chunk 54\n",
      " - chunk shape: 60000\n",
      "Processing chunk 55\n",
      " - chunk shape: 60000\n",
      "Processing chunk 56\n",
      " - chunk shape: 60000\n",
      "Processing chunk 57\n",
      " - chunk shape: 60000\n",
      "Processing chunk 58\n",
      " - chunk shape: 60000\n",
      "Processing chunk 59\n",
      " - chunk shape: 60000\n",
      "Processing chunk 60\n",
      " - chunk shape: 60000\n",
      "Processing chunk 61\n",
      " - chunk shape: 60000\n",
      "Processing chunk 62\n",
      " - chunk shape: 60000\n",
      "Processing chunk 63\n",
      " - chunk shape: 60000\n",
      "Processing chunk 64\n",
      " - chunk shape: 60000\n",
      "Processing chunk 65\n",
      " - chunk shape: 60000\n",
      "Processing chunk 66\n",
      " - chunk shape: 60000\n",
      "Processing chunk 67\n",
      " - chunk shape: 60000\n",
      "Processing chunk 68\n",
      " - chunk shape: 60000\n",
      "Processing chunk 69\n",
      " - chunk shape: 60000\n",
      "Processing chunk 70\n",
      " - chunk shape: 60000\n",
      "Processing chunk 71\n",
      " - chunk shape: 60000\n",
      "Processing chunk 72\n",
      " - chunk shape: 60000\n",
      "Processing chunk 73\n",
      " - chunk shape: 60000\n",
      "Processing chunk 74\n",
      " - chunk shape: 60000\n",
      "Processing chunk 75\n",
      " - chunk shape: 60000\n",
      "Processing chunk 76\n",
      " - chunk shape: 60000\n",
      "Processing chunk 77\n",
      " - chunk shape: 60000\n",
      "Processing chunk 78\n",
      " - chunk shape: 60000\n",
      "Processing chunk 79\n",
      " - chunk shape: 60000\n",
      "Processing chunk 80\n",
      " - chunk shape: 60000\n",
      "Processing chunk 81\n",
      " - chunk shape: 60000\n",
      "Processing chunk 82\n",
      " - chunk shape: 60000\n",
      "Processing chunk 83\n",
      " - chunk shape: 60000\n",
      "Processing chunk 84\n",
      " - chunk shape: 60000\n",
      "Processing chunk 85\n",
      " - chunk shape: 60000\n",
      "Processing chunk 86\n",
      " - chunk shape: 60000\n",
      "Processing chunk 87\n",
      " - chunk shape: 60000\n",
      "Processing chunk 88\n",
      " - chunk shape: 60000\n",
      "Processing chunk 89\n",
      " - chunk shape: 60000\n",
      "Processing chunk 90\n",
      " - chunk shape: 60000\n",
      "Processing chunk 91\n",
      " - chunk shape: 60000\n",
      "Processing chunk 92\n",
      " - chunk shape: 60000\n",
      "Processing chunk 93\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "proc_func = lambda x: TaggedDocument(\n",
    "    words = tokenize_text(x.Title) + tokenize_text(x.Body),\n",
    "    tags = str(x.Tags).split()\n",
    ")\n",
    "jitted = numba.jit(proc_func)\n",
    "\n",
    "for i, chunk in enumerate(traindf):\n",
    "    print(\"Processing chunk %s\" % i)\n",
    "    tagged = chunk.apply(jitted, axis=1)\n",
    "    print(\" - chunk shape: %s\"%tagged.shape)\n",
    "    tagged.to_hdf(\"data/tagged.hdf\",key=\"chunk_%s\"%i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"data/tagged.hdf\", key=\"chunk_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function',\n",
       " 'declaration',\n",
       " 'parameter',\n",
       " 'naming',\n",
       " 'best',\n",
       " 'practices',\n",
       " 'c++',\n",
       " 'in',\n",
       " 'function',\n",
       " 'declaration',\n",
       " 'while',\n",
       " 'the',\n",
       " 'parameters',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'to',\n",
       " 'be',\n",
       " 'named',\n",
       " 'is',\n",
       " 'it',\n",
       " 'preferable',\n",
       " 'to',\n",
       " 'have',\n",
       " 'them',\n",
       " 'named',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'advantages',\n",
       " 'and',\n",
       " 'disadvantages',\n",
       " 'of',\n",
       " 'this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].words"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Facebook_kaggle",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
